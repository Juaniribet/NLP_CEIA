{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Sistema de obtención de información con NLTK utilizando un corpus de wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\juani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\juani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\juani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\juani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos librerias\n",
    "\n",
    "import string\n",
    "import re # Regular Expressions (regex)\n",
    "import nltk\n",
    "import urllib.request\n",
    "import unicodedata\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Para leer y parsear el texto en HTML de wikipedia\n",
    "import bs4 as bs\n",
    "\n",
    "# Descargar el diccionario\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos\n",
    "Se consumirán los datos del artículo de wikipedia sobre la historia Argentina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_html = urllib.request.urlopen('https://es.wikipedia.org/wiki/Historia_de_la_Argentina')\n",
    "raw_html = raw_html.read()\n",
    "\n",
    "# Parsear artículo, 'lxml' es el parser a utilizar\n",
    "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
    "\n",
    "# Encontrar todos los párrafos del HTML (bajo el tag <p>)\n",
    "# y tenerlos disponible como lista\n",
    "article_paragraphs = article_html.find_all('p')\n",
    "\n",
    "article_text = ''\n",
    "\n",
    "for para in article_paragraphs:\n",
    "    article_text += para.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de caracteres en la nota: 243727\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de caracteres en la nota:\", len(article_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Preprocesamiento\n",
    "Definimos la funcion para hacer el preprocesamiento\n",
    "- Sacar tildes de las palabra\n",
    "- Remover caracteres especiales\n",
    "- Quitar espacios o saltos\n",
    "- Quitar caracteres de puntuación (opcional)\n",
    "- Pasa a minúsculas todo el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_clean_text(text, punctuation_removal = False):    \n",
    "    # sacar tildes de las palabras:\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    # quitar caracteres especiales\n",
    "    pattern = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' # [^ : ningún caracter de todos estos\n",
    "    # (termina eliminando cualquier caracter distinto de los del regex)\n",
    "    text = re.sub(pattern, '', text)\n",
    "    # substituir más de un caracter de espacio, salto de línea o tabulación\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # quitar caracteres de puntuación\n",
    "    if punctuation_removal:\n",
    "        text = ''.join([c for c in text if c not in string.punctuation])\n",
    "    # pasa a minúsculas todo el texto\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = preprocess_clean_text(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de caracteres en la nota: 242415\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de caracteres en la nota:\", len(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Dividir el texto en sentencias y en palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus:  1083\n",
      "Vocabulario:  43206\n"
     ]
    }
   ],
   "source": [
    "corpus = nltk.sent_tokenize(article) # divide en oraciones\n",
    "words = nltk.word_tokenize(article) # divide en términos\n",
    "print(\"corpus: \" , len(corpus))\n",
    "print(\"Vocabulario: \", len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Funciones de ayuda para limpiar y procesar el input del usuario\n",
    "- Lematizar los tokens de la oración\n",
    "- Quitar símbolos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def perform_lemmatization(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def get_processed_text(document):\n",
    "    return nltk.word_tokenize(preprocess_clean_text(document, punctuation_removal = True))\n",
    "\n",
    "# # ord() nos da el código Unicode para un caracter dado\n",
    "# punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
    "\n",
    "# def get_processed_text(document):\n",
    "#     # 1 - reduce el texto a mínuscula (string.lower())\n",
    "#     # 2 - quitar los simbolos de puntuacion (string.translate())\n",
    "#     # 3 - realiza la tokenización (nltk.word_tokenize)\n",
    "#     # 4 - realiza la lematización (nuestra función perform_lemmatization)\n",
    "#     return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defino los Stopwords en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', \n",
    " 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', \n",
    " 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', \n",
    " 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', \n",
    " 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', \n",
    " 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', \n",
    " 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', \n",
    "  'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos']\n",
    "\n",
    "for w in stopwords.words('spanish'):\n",
    "    preprocess_clean_text(w)\n",
    "    stopword.append(w)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Utilizar vectores TF-IDF y la similitud coseno construido con el corpus del artículo de wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_response(user_input, corpus):\n",
    "    response = ''\n",
    "    # Sumar al corpus la pregunta del usuario para calcular\n",
    "    # su cercania con otros documentos/sentencias\n",
    "    # la entrada del usuario se usa para tokenizar y vectorizar\n",
    "    corpus.append(user_input)\n",
    "\n",
    "    # Crear un vectorizar TFIDF que quite las \"stop words\" del ingles y utilice\n",
    "    # nuestra funcion para obtener los tokens lematizados \"get_processed_text\"\n",
    "    word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words=stopword)\n",
    "\n",
    "    # Crear los vectores a partir del corpus\n",
    "    all_word_vectors = word_vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Calcular la similitud coseno entre todas los documentos excepto el agregado (el útlimo \"-1\")\n",
    "    # NOTA: con los word embedings veremos más en detalle esta matriz de similitud\n",
    "    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
    "    \n",
    "\n",
    "    # Obtener el índice del vector más cercano a nuestra oración\n",
    "    # --> descartando la similitud contra nuestor vector propio\n",
    "    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
    "    matched_vector = similar_vector_values.flatten()\n",
    "    matched_vector.sort()\n",
    "    vector_matched = matched_vector[-2]\n",
    "\n",
    "    print('cosine similarity value: ', vector_matched)\n",
    "\n",
    "    if vector_matched < 0.2: # si la similaridad coseno fue nula (ningún término en común)\n",
    "        response = \"Disculpame, no tengo esa respuesta\"\n",
    "    else:\n",
    "        response = corpus[similar_sentence_number] # obtener el documento del corpus más similar\n",
    "    \n",
    "    corpus.remove(user_input)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Ensayar el sistema\n",
    "##### Pruebo los resultaods con varios inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juani\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity value:  0.31522622384213456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'en 1816 se declaro la independencia de las provincias unidas de america del sur en el congreso de tucuman.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"cuando fue la independencia\"\n",
    "generate_response(user_input, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity value:  0.3242510519923336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'el 25 de mayo de 1910 se cumplian 100 anos desde la revolucion de mayo, paso inicial de la independencia.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"decime algo sobre la revolución de mayo\"\n",
    "generate_response(user_input, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity value:  0.19318586788165126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Disculpame, no tengo esa respuesta'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"cual fue el decimo presidente\"\n",
    "generate_response(user_input, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity value:  0.1857284372122554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Disculpame, no tengo esa respuesta'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"quien gano la elección en 1916\"\n",
    "generate_response(user_input, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity value:  0.28477675879002606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'una vez que hipolito yrigoyen fue elegido presidente en 1916, comenzo a formarse dentro de la union civica radical un amplio sector que se opuso a yrigoyen, considerandolo autoritario.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"quien era el presidente en 1916\"\n",
    "generate_response(user_input, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juani\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\juani\\AppData\\Local\\Temp\\ipykernel_8636\\1968786774.py:9: GradioDeprecationWarning: `layout` parameter is deprecated, and it has no effect\n",
      "  iface = gr.Interface(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def bot_response(human_text):\n",
    "    print(\"Q:\", human_text)    \n",
    "    resp = generate_response(human_text.lower(), corpus)\n",
    "    print(\"A:\", resp)\n",
    "    return resp\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=bot_response,\n",
    "    inputs=[\"textbox\"],\n",
    "    outputs=\"text\",\n",
    "    layout=\"vertical\")\n",
    "\n",
    "iface.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
